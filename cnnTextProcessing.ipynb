{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnnTextProcessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PopovMihail1/neuralNetworks/blob/master/cnnTextProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9QzPktnYBkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/ikopeykin/googletest/raw/master/train.csv\n",
        "!wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec\n",
        "!wget https://github.com/ikopeykin/googletest/raw/master/test.csv\n",
        "  \n",
        "import pandas\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "data = pandas.read_csv('train.csv', error_bad_lines=False)\n",
        "\n",
        "test_data = pandas.read_csv('test.csv', error_bad_lines=False)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub('\\W', ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\t', ' ', text)\n",
        "    text = re.sub('u', 'you', text)\n",
        "    text = re.sub('ur', 'your', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    text = stemmer.stem(text)\n",
        "    return text\n",
        "\n",
        "def text_to_sent(data):\n",
        "    input_text_noparens = []\n",
        "    for i in range(len(data)):  # Should be range(len(data))\n",
        "        input_text_noparens.append(re.sub(r'\\([^)]*\\)', '', data['comment_text'][i]))\n",
        "\n",
        "    sentences = []    \n",
        "    for sent_str in input_text_noparens:\n",
        "        tokens = re.sub(r\"[^a-z]+\", \" \", clean_text(sent_str.lower())).split()\n",
        "        sentences.append(tokens)\n",
        "        \n",
        "    return sentences\n",
        "\n",
        "emot = np.zeros((len(data['comment_text']), len(LABELS)))\n",
        "for i,label in enumerate(LABELS):\n",
        "    arr = np.array(data[label])\n",
        "    emot[:,i] = arr\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100000, lower=True, char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(text_to_sent(data) + text_to_sent(test_data))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(100000, len(word_index))\n",
        "\n",
        "word_seq_train = tokenizer.texts_to_sequences(text_to_sent(data))\n",
        "\n",
        "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=2500)\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_epochs = 8 \n",
        "num_filters = 32\n",
        "optimizer = 'adam'\n",
        "\n",
        "embed_dim = 300\n",
        "\n",
        "early_stopping=EarlyStopping(monitor='value_loss')\n",
        "\n",
        "def build_cnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(nb_words+1, embed_dim, input_length=2500, trainable=False))\n",
        "    model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(6, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "train_text_set, valid_text_set, train_emot_set, valid_emot_set = train_test_split(word_seq_train, emot, \n",
        "                                                                                  test_size=0.1,\n",
        "                                                                                  shuffle=True)\n",
        "\n",
        "model = build_cnn_model()\n",
        "hist = model.fit(train_text_set, train_emot_set, \n",
        "                 batch_size=batch_size, epochs=num_epochs,\n",
        "                 shuffle=True, verbose=2,\n",
        "                 callbacks=[early_stopping])\n",
        "\n",
        "predicted_values = model.predict(valid_text_set, verbose=True)\n",
        "\n",
        "for i,label in enumerate(LABELS):\n",
        "  score = roc_auc_score(valid_emot_set[:,i], predicted_values[:,i])\n",
        "  print('Result for ', label, ' is ', score)\n",
        "\n",
        "\n",
        "sent = text_to_sent(test_data)\n",
        "word_seq_train = tokenizer.texts_to_sequences(sent)\n",
        "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=2500)\n",
        "\n",
        "results = np.zeros((len(test_data), 6))\n",
        "\n",
        "results = model.predict(word_seq_train, verbose=False)\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('results.csv', 'w', newline='') as csv_file:\n",
        "    fieldnames=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    csv_writer.writeheader()\n",
        "    for i,item in enumerate(results):\n",
        "        to_push = {fieldnames[0]:test_data['id'][i],\n",
        "                   fieldnames[1]:item[0],\n",
        "                   fieldnames[2]:item[1],\n",
        "                   fieldnames[3]:item[2],\n",
        "                   fieldnames[4]:item[3],\n",
        "                   fieldnames[5]:item[4],\n",
        "                   fieldnames[6]:item[5]\n",
        "                  }\n",
        "        csv_writer.writerow(to_push)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('results.csv')\n",
        "\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "!ls -lr\n",
        "\n",
        "files.download('model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}